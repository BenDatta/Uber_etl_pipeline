# ğŸš– Uber ETL Pipeline with Airflow & GCS  

## ğŸ“Œ Overview  
This project demonstrates an **end-to-end ETL pipeline** for Uber ride data using **Apache Airflow**.  
The pipeline:  
- ğŸ“¥ Extracts raw CSV data  
- ğŸ§¹ Transforms it into a clean format  
- â˜ï¸ Loads it into **Google Cloud Storage (GCS)** for further analytics & visualization  

This project highlights my **Data Engineering skills** in:  
âœ”ï¸ Pipeline automation  
âœ”ï¸ Data cleaning  
âœ”ï¸ Cloud integration  
âœ”ï¸ Data Analysis and Visualization

*(Click image to view interactive dashboard)* 
[![Uber ETL Dashboard](https://raw.githubusercontent.com/BenDatta/Uber_etl_pipeline/main/dashboard.png)](https://lookerstudio.google.com/s/k9dSeSe-nJk)

---

## âš™ï¸ Tech Stack  

- ğŸ **Python** â†’ Core ETL scripting & transformations  
- ğŸ“¦ **Pandas** â†’ Data cleaning and wrangling  
- â˜ï¸ **Google Cloud Storage (GCS)** â†’ Cloud data lake  
- ğŸ› ï¸ **Apache Airflow** â†’ Orchestration & workflow automation  
- ğŸ³ **Docker (Optional)** â†’ Containerized Airflow deployment  
- ğŸ“Š **Looker Studio / Power BI** â†’ Analytics & dashboards  

---

ğŸ“¥ **Extract**  
- Read raw Uber CSV dataset from local storage  
- Validate file presence & row count  

ğŸ§¹ **Transform**  
- Drop unused/unnecessary columns  
- Fill missing values with median/mode strategies  
- Standardize column names & output cleaned dataset  

â˜ï¸ **Load**  
- Upload transformed data as **`uber_cleaned.csv`** into GCS bucket  

âš™ï¸ **Orchestration**  
- Airflow DAG runs daily with retry logic  
- Uses **XCom** to pass metadata between tasks  

---

## ğŸ“Š Example Use Cases
With the cleaned dataset stored in GCS, you can build:

ğŸš– Trip Analytics â†’ Trips by day, city, time of day  
ğŸ’µ Revenue Reports â†’ Average booking value & total revenue  
ğŸŒ Geospatial Analysis â†’ Pick-up and drop-off heatmaps  
â­ Driver Performance â†’ Ratings, cancellations, and service quality  

---

## ğŸ”§ Code Walkthrough
DAG File â†’ `uber_etl_dag.py`  

extract_data() â†’ Reads the raw CSV file  
transform_data() â†’ Cleans, imputes, and restructures data  
load_to_gcs() â†’ Uploads cleaned file to GCS bucket  

Airflow DAG Flow:  
extract_task >> transform_task >> load_task  

â¡ï¸ Ensures correct order: **Extract â†’ Transform â†’ Load**

---

## ğŸš€ How to Run

1ï¸âƒ£ **Clone Repository**  
git clone https://github.com/BenDatta/Uber_etl_pipeline.git  
cd Uber_etl_pipeline  

2ï¸âƒ£ **Install Requirements**  
pip install -r requirements.txt  

3ï¸âƒ£ **Setup Google Cloud**  
- Create a GCS bucket (e.g., `uber_data_etl`)  
- Authenticate with a GCP service account:  
```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/key.json"e.g., uber_data_etl)

Authenticate with a GCP service account:

export GOOGLE_APPLICATION_CREDENTIALS="/path/to/key.json"

4ï¸âƒ£ Run Airflow DAG

Start Airflow:

airflow standalone


Enable DAG: uber_etl_dag
Check logs for ETL execution ğŸš€

ğŸ¯ Key Highlights

âœ… Production-ready ETL pipeline with Airflow
âœ… Data cleaning & quality checks using Pandas
âœ… Automated scheduling with Airflow DAGs
âœ… Integrated with Google Cloud Storage for scalable storage
âœ… Designed for real-world analytics & BI dashboards
